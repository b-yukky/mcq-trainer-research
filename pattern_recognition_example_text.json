["\u203a Supervised method for dimensionality reduction\n\u203a Attempt to optimize class separability\n\u203a Canonicals\n\u2013 Linear combination of the original features\n\u2013 Found by maximizing the Fisher discriminant ratio (FDR)\nFDR = \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd1\n2 \u2212 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd2\n2\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd1\n2 + \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd2\n2\n* In the case of two-class problem", "\u203a Total invariance is conserved\n\u2013 Sum of the variances of the principal components is equal \nto the sum of the variances (trace(\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd)) of the original \nvariables\n\u203a The contribution of any eigenvalues \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd to the original \nvariance is \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd/trace(\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd)\n\u203a If the original variables are correlated\u2026\n\u2013 A small number of eigenvectors with large eigenvalues\n\u2013 A large reduction in dimensionality can be obtained by \nkeeping only the \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd largest principal components", "\u203a Complexity of classification depends on\n\u2013 The number of features\n\u2013 The number of data samples \n\u203a Curse of dimensionality\n\u2013 As the dimensionality increases, increasing amounts of \ntraining data are required\n\u203a The number of features should be reduced\n\u2013 To reduce the computational complexity\n\u2013 The data can be analyzed visually more easily", "\u203a Parametric method\n\u2013 Parameter estimation for representation of probability \ndistributions\n\u2013 Discriminant analysis for separating the classes\n\u2013 Slow at training, but fast at classifying test data\n\u203a Non-parametric method\n\u2013 In the case that the probability distribution is not known\n\u2013 Estimation of density functions\n\u2013 Or, directly constructing decision boundaries", "\u203a Simplest but basic approach\n\u2013 Divide an image into 2 parts\n\u203a dark regions and bright regions\n\u2013 Assumption\n\u203a the image consists of two major parts: dark and bright\n\u2252foreground and background\n\u203a Basic idea\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd, \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd = \ufffd1 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd, \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \u2265 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n0 \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd, \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd < \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd: threshold", "\u203a Construction of a scale space\n\u2013 To make sure that features are scale invariant\n\u203a Keypoint Localization\n\u2013 Identifying the suitable features or keypoints\n\u203a Orientation Assignment\n\u2013 Ensure the keypoints are rotation invariant\n\u203a Keypoint Descriptor\n\u2013 Assign a unique fingerprint to each keypoint", "\u203a Unsupervised learning\n\u2013 The class labels are unknown\n\u2013 The data are plotted to see whether it clusters naturally\n\u203a Cluster analysis\n\u2013 Divide the data into clusters\n\u2013 The clusters may or may not correspond with the human \nperception of similarity ", "\u203a Most widely used clustering algorithms\n\u203a Partitional clustering approach\n\u203a Each cluster is represented by one prototype object\n\u203a A new data sample is assigned to the nearest \nprototype, i.e., to that cluster", "\u203a No guarantee for convergence to the global optimum\n\u203a Sensitive to the initial choice of objects\n\u203a Greedy algorithm for partitioning \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd samples into \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\nclusters so as to minimize an objective function: the \nsum of the squared error (SSE)\n\u203a \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd: the center of the \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdth cluster \nSSE = \ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd=1\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\n\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\u2208\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ndist \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd, \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd 2", "1. Find the two features that are closest in \nmultivariate space\n2. Replace them with a single feature at their mean\n3. Repeat with the next two closest features, and \ncontinue until the features are subsumed into one \ncluster\n\u203a How to find the closest features?\n\u2013 Single-link clustering\n\u2013 Complete-link clustering"]