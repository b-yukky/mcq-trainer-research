{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pattern_recognition_example_text.json\",'r') as fin:\n",
    "    example_text= json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['› Supervised method for dimensionality reduction\\n› Attempt to optimize class separability\\n› Canonicals\\n– Linear combination of the original features\\n– Found by maximizing the Fisher discriminant ratio (FDR)\\nFDR = ������������1\\n2 − ������������2\\n2\\n������������1\\n2 + ������������2\\n2\\n* In the case of two-class problem',\n",
       " '› Total invariance is conserved\\n– Sum of the variances of the principal components is equal \\nto the sum of the variances (trace(������������)) of the original \\nvariables\\n› The contribution of any eigenvalues ������������������������ to the original \\nvariance is ������������������������/trace(������������)\\n› If the original variables are correlated…\\n– A small number of eigenvectors with large eigenvalues\\n– A large reduction in dimensionality can be obtained by \\nkeeping only the ������������ largest principal components',\n",
       " '› Complexity of classification depends on\\n– The number of features\\n– The number of data samples \\n› Curse of dimensionality\\n– As the dimensionality increases, increasing amounts of \\ntraining data are required\\n› The number of features should be reduced\\n– To reduce the computational complexity\\n– The data can be analyzed visually more easily',\n",
       " '› Parametric method\\n– Parameter estimation for representation of probability \\ndistributions\\n– Discriminant analysis for separating the classes\\n– Slow at training, but fast at classifying test data\\n› Non-parametric method\\n– In the case that the probability distribution is not known\\n– Estimation of density functions\\n– Or, directly constructing decision boundaries',\n",
       " '› Simplest but basic approach\\n– Divide an image into 2 parts\\n› dark regions and bright regions\\n– Assumption\\n› the image consists of two major parts: dark and bright\\n≒foreground and background\\n› Basic idea\\n������������ ������������, ������������ = �1 ������������ ������������, ������������ ≥ ������������\\n0 ������������ ������������, ������������ < ������������\\n������������: threshold',\n",
       " '› Construction of a scale space\\n– To make sure that features are scale invariant\\n› Keypoint Localization\\n– Identifying the suitable features or keypoints\\n› Orientation Assignment\\n– Ensure the keypoints are rotation invariant\\n› Keypoint Descriptor\\n– Assign a unique fingerprint to each keypoint',\n",
       " '› Unsupervised learning\\n– The class labels are unknown\\n– The data are plotted to see whether it clusters naturally\\n› Cluster analysis\\n– Divide the data into clusters\\n– The clusters may or may not correspond with the human \\nperception of similarity ',\n",
       " '› Most widely used clustering algorithms\\n› Partitional clustering approach\\n› Each cluster is represented by one prototype object\\n› A new data sample is assigned to the nearest \\nprototype, i.e., to that cluster',\n",
       " '› No guarantee for convergence to the global optimum\\n› Sensitive to the initial choice of objects\\n› Greedy algorithm for partitioning ������������ samples into ������������\\nclusters so as to minimize an objective function: the \\nsum of the squared error (SSE)\\n› ������������������������: the center of the ������������th cluster \\nSSE = �\\n������������=1\\n������������\\n�\\n������������∈������������������������\\ndist ������������������������, ������������ 2',\n",
       " '1. Find the two features that are closest in \\nmultivariate space\\n2. Replace them with a single feature at their mean\\n3. Repeat with the next two closest features, and \\ncontinue until the features are subsumed into one \\ncluster\\n› How to find the closest features?\\n– Single-link clustering\\n– Complete-link clustering']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ds = Dataset.from_dict({\"id\": ['']*len(example_text)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'context', 'answer'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = dataset_ds.add_column(name=\"context\", column=example_text)\n",
    "examples.add_column(name=\"answer\", column=['']*len(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fceee184c6517569c17b01ab85c96ed0a3e21c197e837da8a26bf896b5cefc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
